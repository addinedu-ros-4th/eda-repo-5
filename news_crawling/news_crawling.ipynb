{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(query, start_date, end_date, start_page):\n",
    "    # 네이버 뉴스검색에서 검색할 뉴스의 카테고리, 시간 범위 옵션을 설정했을 때의 url을 생성\n",
    "    base = \"https://search.naver.com/search.naver?where=news&query={search_words}\"\n",
    "    ds = start_date\n",
    "    de = end_date\n",
    "    start = start_page\n",
    "    url = base + \"&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=\" + ds + \"&de=\" + de + \"&start=\" + start\n",
    "    request_url = Request(url.format(search_words = urllib.parse.quote(query)))\n",
    "    return request_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_selenium_opt():\n",
    "    # 크롤링을 하기 위한 selenlium 크롬 드라이버의 설정값\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # 브라우저를 표시하지 않음\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--dns-prefetch-disable\")\n",
    "    chrome_options.add_argument(\"--disable-logging\")\n",
    "    chrome_options.add_argument(\"--disable-background-networking\")\n",
    "    chrome_options.add_argument(\"--disable-default-apps\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_html(driver, url):\n",
    "    # 해당 url 페이지의 전체 html 추출\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'title')))\n",
    "    page_html = driver.page_source\n",
    "    # print(page_html)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "def filter_naver_links(links):\n",
    "    # 추출한 링크들 중 네이버 뉴스 링크들만 선별\n",
    "    return [link for link in links if 'news.naver.com' in link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_link_crawling(driver, url):\n",
    "    # 특정 키워드, 범위 안의 네이버 기사들을 크롤링\n",
    "    \n",
    "    link_list = []\n",
    "\n",
    "    # 해당 url 페이지에서 추출한 html이 제대로 존재하는지 여부를  확인\n",
    "    while True:\n",
    "        html_content = get_page_html(driver, url.full_url)\n",
    "\n",
    "        if html_content:\n",
    "            break\n",
    "        print(\"Warning!!!\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # BeautifulSoup을 사용하여 페이지 파싱\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    group = soup.find(\"ul\", \"list_news\")\n",
    "    \n",
    "    if not group:\n",
    "        return []\n",
    "    \n",
    "    bx = group.find_all(\"li\", \"bx\")\n",
    "    for li_tag in bx:\n",
    "        a_tags = li_tag.find_all(\"a\")\n",
    "        for a_tag in a_tags:\n",
    "            link_list.append(a_tag[\"href\"])\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_article_crawling(driver, url):\n",
    "    \n",
    "    date = []\n",
    "    title = []\n",
    "    text = []\n",
    "    \n",
    "    for row in url:\n",
    "        # 태그 형식이 하나라도 다르다면 NAN값을 집어넣어줌\n",
    "        try:\n",
    "            driver.get(row)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"h2\")))\n",
    "        \n",
    "            date.append(driver.find_element(By.CLASS_NAME, \"media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\").get_attribute(\"data-date-time\"))\n",
    "            title.append(driver.find_element(By.ID, \"title_area\").get_attribute(\"innerText\"))\n",
    "            text.append(driver.find_element(By.ID, \"dic_area\").get_attribute(\"innerText\"))\n",
    "        except NoSuchElementException:\n",
    "            date.append(np.NAN)\n",
    "            title.append(np.NAN)\n",
    "            text.append(np.NAN)\n",
    "            \n",
    "    \n",
    "    return pd.DataFrame({\"date\" : date, \"title\" : title, \"text\" : text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refined_article(article):\n",
    "    \n",
    "    # 중복된 기사 삭제\n",
    "    article = article.drop_duplicates([\"date\", \"text\"], keep = \"first\")\n",
    "    \n",
    "    # 기사 내용이 없는 데이터의 경우 삭제처리\n",
    "    article[article[\"text\"] == \"nan\"] = np.nan\n",
    "    article.dropna(subset=[\"text\"],how=\"any\", axis = 0, inplace=True)\n",
    "\n",
    "    # 제목에서 문자, 스페이스만 남기고 제거\n",
    "    article[\"title\"] = article[\"title\"].apply(lambda x : re.sub(\"[^A-Za-z0-9가-힣]\", \" \", x))\n",
    "    article[\"title\"] = article[\"title\"].apply(lambda x : re.sub(\" +\", \" \", x))\n",
    "\n",
    "    # 내용에서 문자, 스페이스만 남기고 제거\n",
    "    article[\"text\"] = article[\"text\"].apply(lambda x : re.sub(\"[^A-Za-z0-9가-힣.]\", \" \", x))\n",
    "    article[\"text\"] = article[\"text\"].apply(lambda x : re.sub(\" +\", \" \", x))\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(year, month, term, topic):\n",
    "    link_list = []\n",
    "    news_info_list = []\n",
    "\n",
    "    try:\n",
    "        start_date = datetime.strptime(f\"{year}.{month:02d}.01\", \"%Y.%m.%d\")\n",
    "        end_date = datetime.strptime(f\"{year}.{month:02d}.31\", \"%Y.%m.%d\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        if month == 2:\n",
    "            end_date = datetime.strptime(f\"{year}.{month:02d}.28\", \"%Y.%m.%d\")\n",
    "        else:\n",
    "            end_date = datetime.strptime(f\"{year}.{month:02d}.30\", \"%Y.%m.%d\")\n",
    "\n",
    "    current_date = start_date\n",
    "    formatted_date = str(current_date.strftime(\"%Y.%m.%d\"))\n",
    "    \n",
    "    print(f\"Crawling {start_date} to {end_date}\")\n",
    "    start_page = 1\n",
    "\n",
    "    driver = set_selenium_opt()\n",
    "        \n",
    "    while current_date <= end_date:\n",
    "        formatted_date = str(current_date.strftime(\"%Y.%m.%d\"))\n",
    "\n",
    "        for start_page in range(1, 101, 10):\n",
    "            url = get_url(f\"{topic}\", str(start_date.strftime(\"%Y.%m.%d\")), formatted_date, str(start_page))\n",
    "            link = do_link_crawling(driver, url)\n",
    "            link_list.extend(link)\n",
    "        current_date += timedelta(days=term)\n",
    "        \n",
    "    # 네이버 링크만 남겨주기\n",
    "    naver_link_list = filter_naver_links(link_list)\n",
    "    \n",
    "    # 얻은 링크를 바탕으로 기사정보를 크롤링  \n",
    "    news_info_list = do_article_crawling(driver, naver_link_list)\n",
    "    \n",
    "    # 특수문자 혹은 불필요한 문자 삭제\n",
    "    refined_news_info = get_refined_article(news_info_list)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    # 형태소 분리 후 기사 데이터프레임에 형태소 열 추가\n",
    "    # refined_news_info[\"title_tokenized\"] = get_tokenized_article(refined_news_info[\"title\"])\n",
    "    # refined_news_info[\"text_tokenized\"] = get_tokenized_article(refined_news_info[\"text\"])\n",
    "    # refined_news_info[\"title_tokenized\"] = None\n",
    "    # refined_news_info[\"text_tokenized\"] = None    \n",
    "    \n",
    "    # print(naver_link_list)\n",
    "    df = pd.DataFrame(refined_news_info)\n",
    "    # df.to_csv(f\"../news_crawling/data/{topic.replace(' ', '')}/{topic.replace(' ', '')}_{year}_{month:02d}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tyear = 2023\n",
    "\tterm = 10\n",
    "\ttopic = '전기차'\n",
    "\t\n",
    "\tdirectory_path = f\"../news_crawling/data/{topic.replace(' ', '')}\"\n",
    "\ttry:\n",
    "\t\tos.mkdir(directory_path)\n",
    "\texcept FileExistsError:\n",
    "\t\tprint(f\"The directory '{directory_path}' already exists.\")\n",
    "\n",
    "\tfor i in range(1,12,1):\n",
    "\t\tmonth = i\n",
    "\t\tmain(year, month, term, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
