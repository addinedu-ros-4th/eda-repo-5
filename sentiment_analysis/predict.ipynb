{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import re\n",
    "import shutil\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "sys.path.append(f'{cur_path}/KoBERT/')\n",
    "# print(cur_path + \"/KoBERT\")\n",
    "\n",
    "#kobert\n",
    "from KoBERT.kobert.utils import get_tokenizer\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW, BertModel\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 128\n",
    "batch_size = 22\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=3,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, tok, text_list):\n",
    "    value_dict = {'중립': 0.0, '호재': 0.0, '악재': 0.0}\n",
    "    logits_list = []\n",
    "    test_dataloader_list = []\n",
    "\n",
    "    if str(device) == 'cuda:0':\n",
    "        model.to(device)\n",
    "\n",
    "    # 각 문장에 대한 test_dataloader_list 생성\n",
    "    for sentence in text_list:\n",
    "        data = [sentence, '0']\n",
    "        dataset_another = [data]\n",
    "        another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "        test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "        test_dataloader_list.append(test_dataloader)\n",
    "    # 각 문장 test_dataloader 계산 -> 수치화?\n",
    "    for test_dataloader in tqdm(test_dataloader_list):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            logits_list.extend(out.detach().cpu().numpy())\n",
    "\n",
    "    count_value = [0,0,0]\n",
    "    # # 결과 처리\n",
    "    for logit in logits_list:\n",
    "        if np.argmax(logit) == 0:\n",
    "            count_value[0] += 1\n",
    "            value_dict['중립'] += np.max(logit) * 0.33\n",
    "        elif np.argmax(logit) == 1:\n",
    "            count_value[1] += 1\n",
    "            value_dict['호재'] += np.max(logit)\n",
    "        elif np.argmax(logit) == 2:\n",
    "            count_value[2] += 1\n",
    "            value_dict['악재'] += np.max(logit)\n",
    "    # print(count_value)\n",
    "    # print(value_dict)\n",
    "    # print(max(value_dict.values()) / (value_dict['중립'] + value_dict['호재'] + value_dict['악재']))\n",
    "\n",
    "    max_value_rate = max(value_dict.values()) / (value_dict['중립'] + value_dict['호재'] + value_dict['악재'])\n",
    "    result = {'predict': max(value_dict, key=value_dict.get), 'score': max_value_rate}\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "def use_gpu():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "\n",
    "def load_model(model_path, model_name):\n",
    "    load_model = torch.load(f'{model_path}{model_name}model.pt')\n",
    "    load_model.load_state_dict(torch.load(f'{model_path}{model_name}weights.pt'))\n",
    "    checkpoint = torch.load(f'{model_path}{model_name}model.tar')   \n",
    "    load_model.load_state_dict(checkpoint['model'])\n",
    "    return load_model\n",
    "\n",
    "def load_data(path, file_name):\n",
    "    df = pd.read_csv(f'{path}/{file_name}.csv')\n",
    "    df['text'] = df['text'].apply(lambda x: [paragraph.strip() + '.' for paragraph in x.split('.') if paragraph.strip()])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    common_path = f'../sentiment_analysis'\n",
    "    model_path = f\"{common_path}/models/\"\n",
    "    model_name = \"kobert/202401230936_model/\"\n",
    "\n",
    "    data_path = f'{common_path}/../news_crawling/data'\n",
    "    company = 'CES'\n",
    "    csv_directory = f'{data_path}/{company}'\n",
    "\n",
    "    # data_name = [\"test_news\"]\n",
    "\n",
    "    data_name = [file[:-4] for file in os.listdir(csv_directory) if file.endswith('.csv')]\n",
    "    data_name.sort()\n",
    "    # print(data_name)\n",
    "\n",
    "    used_data_path = f\"{csv_directory}/used\"\n",
    "    try:\n",
    "        os.mkdir(used_data_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    save_directory_path = f\"{common_path}/data/predicted/{company}\"\n",
    "    try:\n",
    "        os.mkdir(save_directory_path)\n",
    "    except FileExistsError:\n",
    "        # print(f\"The directory '{save_directory_path}' already exists.\")\n",
    "        pass\n",
    "\n",
    "    device = use_gpu()\n",
    "    model = load_model(model_path, model_name)\n",
    "\n",
    "    bertmodel, vocab = get_pytorch_kobert_model()\n",
    "    tokenizer = get_tokenizer()\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "   \n",
    "\n",
    "    for name in data_name:\n",
    "        try:\n",
    "            df = load_data(csv_directory, name)\n",
    "            predict_result = []\n",
    "            df['prediction'] = None\n",
    "            df['score'] = None\n",
    "\n",
    "            for idx, summary in tqdm(enumerate(df['text']), total=len(df), desc=f\"Processing {name}\"):\n",
    "                predict_result.append(predict(model, device, tok, summary))\n",
    "\n",
    "            # Update the entire dataframe\n",
    "            for update_idx, data in enumerate(predict_result):\n",
    "                df.loc[update_idx, 'prediction'] = data['predict']\n",
    "                df.loc[update_idx, 'score'] = data['score']\n",
    "\n",
    "            # Save the final dataframe\n",
    "            df.to_csv(f'{save_directory_path}/predicted_{name}.csv', index=False)\n",
    "            shutil.move(f'{csv_directory}/{name}.csv', used_data_path)\n",
    "            print(f'{name} Predict and Save Done.')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {name}: {e}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
